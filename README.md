# MMA-Thesis-Projects-2025-26

## Table of contents

- [Who We Are](#who-we-are)
- [Speech Projects](#speech-projects-%EF%B8%8F)
- [Visual/AIGC Projects](#visual-projects-)
- [Join Us](#join-us)

## Who We Are

We are the MultiModal Amsterdam (MMA) Team at the Huawei Technologies Amsterdam Research Center. Our team focuses on audio visual deep learning. Everyone on the team holds a PhD and/or has multiple years of experience in AI, CS, or closely related fields. We provide high quality supervision and mentorship throughout your thesis. Read about the current speech and video/AIGC topics below.

## Speech Projects üéôÔ∏è

### 1. Controllable TTS with Flexible Speaker Identity

- RQ: How can we modify a speaker‚Äôs timbre with minimal additional data (~30s audio) while retaining fine-grained control over emotion, style, and prosody?

This project focuses on next-generation controllable TTS systems, where flexible speaker adaptation and fine-grained control over expressive speech are combined, also with style/voice disentanglement methods.

### 2. Expressivity-Preserving Voice Conversion

- RQ: How can voice conversion change speaker voice/timbre only, without compromising emotion, style, accent, or prosody?

In this topic, you will study techniques to achieve high-fidelity voice conversion that changes speaker ID but keeps emotion/style/accent‚Äîvia content‚Äìspeaker disentanglement, neural codecs, and diffusion vocoders.

### 3. Voice-Only Multimodal Generation (Voice-only VEO3/Sora2)

- RQ: How can we generate rich audio scenes‚Äîspeech plus non-speech sounds‚Äîdirectly from voice or text prompts?

The goal is to generate full audio scenes (speech + SFX/ambience) from voice or text prompts with diffusion/transformers and neural audio codecs

### 4. Cross-Lingual Dubbing for Drama Shorts

- RQ: How can we clone a speaker‚Äôs voice and style across languages under noisy, complex conditions (music, SFX, overlap) while preserving timing and expressivity?

This project explores cross-lingual voice cloning TTS and dubbing, with applications in media production and entertainment. It requires research into multilingual speech synthesis, noise robustness, and expressive voice cloning.

### 5. Real-time Speech LLM with Tool-using

- RQ: How can a speech/spoken LLM call plan and execute external tool calls mid-conversation with low latency, without disrupting natural turn-taking or speech fluency?

The candidate will investigate ways to integrate speech-based LLMs with real-time external tool usage, ensuring natural, continuous spoken interaction. This lies at the intersection of CoT, function calling, dialogue TTS, and low latency hand offs.

### 6. Streaming Reasoning for Real-time Speech LLMs

- RQ: How can a spoken LLM perform CoT reasoning to improve task performance (both semantic-wise and acoustic-wise) while maintaining fluent, real-time streaming dialogue.

This project aims to enable spoken language models that ‚Äúthink while they talk.‚Äù The challenge is to maintain both reasoning depth and conversational fluency. The candidate will research architectures for streaming reasoning within spoken LLMs.

## Visual Projects üé¨

### 7. World Modeling with Video Generation Architectures

- RQ1: How can we maintain long‚Äëhorizon coherence, realistic physics, and agent interactivity in generated worlds?
- RQ2: How can we improve the consistency and realism of simulated worlds over longer durations by building on foundation world models?

Scope: Interactive video worlds from prompts‚Äîtransformer/diffusion video, long horizon memory, simple physics priors, and agent in the loop.

### 8. Audio/Video Synchronization and Joint Generation

- RQ1: How can we generate synchronized audio (SFX, ambience, music, dialogue) that aligns with on‚Äëscreen events and lip movements?
- RQ2: How can we design joint audio‚Äëvisual generators (or V2A pipelines) that maintain temporal/alignment fidelity in fast‚Äëmoving scenes and complex edits?

Scope: Make videos with native sound that lines up‚ÄîAV transformers or V2A diffusion for SFX/music/dialogue and lip sync.

### 9. Multiview Video Enhancement & Restoration with Video Diffusion Models

- RQ1: How can diffusion‚Äëbased enhancers improve novel‚Äëview fidelity and cross‚Äëview consistency for multi‚Äëcamera or 3D reconstruction pipelines?
- RQ2: How can we integrate single‚Äëstep/fast diffusion into the reconstruction loop (e.g., NeRF/gaussians) for artifact removal, super‚Äëresolution, and color/temporal consistency?

Scope: Fix multi view/novel view artifacts‚Äîdiffusion enhancers with NeRF/3DGS for detail, super res, and color/temporal consistency.

## We are looking for
- Master‚Äôs student in AI, CS or related field
- Strong interest in AIGC / speech / audio / multimodality LLM 
- Familiarity with Python and machine learning frameworks (e.g. PyTorch)
- Basic experience with LLM, speech synthesis or diffusion models

## Additional Information:
- Students are required to sign a tripartite internship agreement (university‚Äìcompany‚Äìstudent).
- Internship allowance: ‚Ç¨750 / month.
- Duration: 3‚Äì6 months

## Join Us
We look forward to welcoming you into our team! If one (or more) of these research topics fits your interests, please contact us. We will arrange an interview with you, and if successful, you will have the opportunity to work with us and carry out your thesis project in our Amsterdam Research Center.

- üìç Thesis Fair: Tuesday, Oct 7, 2025, from 13:00‚Äì16:00 @ VU
- üìß Email: tian.xia2@huawei-partners.com
- Please also include the number of the project you‚Äôre most interested  (max 2) in either in your email or in your CV/resume file name.

